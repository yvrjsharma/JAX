{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "JAX_3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPNp4tS7IjFOyU+c28ll7wu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yvrjsharma/JAX/blob/main/JAX_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# JAX: PMaps"
      ],
      "metadata": {
        "id": "JGT5gMrmnvR1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "JAX has lots of cool features to evaluate your code parallely. This is also called SPMD, or Single-Program Multiple-Data code. In this technique same code or computation is run in parallel on different input data on different devices (e.g. TPUs)\n",
        "\n",
        "You can use pmap() to write a piece of code suitable for running on one device as well as on multiple devices.\n",
        "\n"
      ],
      "metadata": {
        "id": "EQJMUGsm0ZQt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets start using TPUs for this one. If you are using this Colab notebook, make sure you change your Runtime to TPU."
      ],
      "metadata": {
        "id": "iNH0-tjBn1gC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Lets import the required libraries\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "\n",
        "#transformation : higher order functions which take fuun as an input and outputs a transformed fun \n",
        "from jax import grad, jit, vmap, pmap\n",
        "from jax import random\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from copy import deepcopy\n",
        "from typing import Tuple, NamedTuple\n",
        "import functools"
      ],
      "metadata": {
        "id": "iar5nrp18j5p"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.tools.colab_tpu\n",
        "jax.tools.colab_tpu.setup_tpu()"
      ],
      "metadata": {
        "id": "91_VTK8y4jAu"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "jax.devices() #Eight devices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Qcoby81odla",
        "outputId": "f0560537-83fb-4623-f3d8-12c7fbfa60ca"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0),\n",
              " TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1),\n",
              " TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0),\n",
              " TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1),\n",
              " TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0),\n",
              " TpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1),\n",
              " TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0),\n",
              " TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to ustilize all the cores that are available to us, lets create some array with batch size equal to the number of cores available.\n",
        "\n",
        "Lets now perform a dummy computation - convolving a small array over this array which is spread across the cores. This dummy example will help us in understanding later how pmap() helps in speeding up more complex or bigger computations.  "
      ],
      "metadata": {
        "id": "QT9QI_hjLfY0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_devices = jax.local_device_count()\n",
        "x = np.arange(5)\n",
        "xs = np.arange(5*n_devices).reshape(-1,5)  #40 unique values, arranged in 5 columns and 8 rows\n",
        "w = np.array([2.,3.,4.])  #for convolving over the given metric\n",
        "ws = np.stack([w] * n_devices)  #duplicating the same convolution kernel on 8 cores\n",
        "x,w , xs, ws"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xo6AsUrC8MIL",
        "outputId": "c4099362-77b8-4298-9e4f-386c96a7afda"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0, 1, 2, 3, 4]), array([2., 3., 4.]), array([[ 0,  1,  2,  3,  4],\n",
              "        [ 5,  6,  7,  8,  9],\n",
              "        [10, 11, 12, 13, 14],\n",
              "        [15, 16, 17, 18, 19],\n",
              "        [20, 21, 22, 23, 24],\n",
              "        [25, 26, 27, 28, 29],\n",
              "        [30, 31, 32, 33, 34],\n",
              "        [35, 36, 37, 38, 39]]), array([[2., 3., 4.],\n",
              "        [2., 3., 4.],\n",
              "        [2., 3., 4.],\n",
              "        [2., 3., 4.],\n",
              "        [2., 3., 4.],\n",
              "        [2., 3., 4.],\n",
              "        [2., 3., 4.],\n",
              "        [2., 3., 4.]]))"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Simple convolution example \n",
        "def convolve(w,x):\n",
        "  output = []\n",
        "  for i in range(1,len(x)-1):\n",
        "    output.append(jnp.dot(x[i-1:i+2],w))\n",
        "  return jnp.array(output)\n",
        "\n",
        "convolve(w,x) #smaller array is convolved over another one"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uow4dnowMt6s",
        "outputId": "544a4045-657b-4781-f817-37c33359198b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray([11., 20., 29.], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Lets first try the convolve operation over xs using just one device\n",
        "#We can do so using vmap()\n",
        "#convolve(ws,xs) #Error for incompatible shapes\n",
        "jax.vmap(convolve)(ws,xs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIUsngIZDW0U",
        "outputId": "2609b45b-ff4f-4cce-f587-c1b3dd6cabcc"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray([[ 11.,  20.,  29.],\n",
              "             [ 56.,  65.,  74.],\n",
              "             [101., 110., 119.],\n",
              "             [146., 155., 164.],\n",
              "             [191., 200., 209.],\n",
              "             [236., 245., 254.],\n",
              "             [281., 290., 299.],\n",
              "             [326., 335., 344.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Coolest thing in JAX is that you don't hav to change the function \n",
        "#When you need to run it on multiple devices\n",
        "#Using pmap() this time\n",
        "jax.pmap(convolve)(ws,xs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_INhgO2GEKcd",
        "outputId": "08de915d-f388-4fcb-812c-6694ed995a82"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ShardedDeviceArray([[ 11.,  20.,  29.],\n",
              "                    [ 56.,  65.,  74.],\n",
              "                    [101., 110., 119.],\n",
              "                    [146., 155., 164.],\n",
              "                    [191., 200., 209.],\n",
              "                    [236., 245., 254.],\n",
              "                    [281., 290., 299.],\n",
              "                    [326., 335., 344.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Another way of writing above code\n",
        "#Using in_axes argument of pmap() \n",
        "#Works similar to vmap()\n",
        "jax.pmap(convolve,in_axes=(None,0))(w,xs)  #Using w instead of ws\n",
        "\n",
        "#'None' would mean to broadcast weight w across all the cores \n",
        "#'0' would mean that 0 dimension in xs matrix is the batch argument\n",
        "#So, pmap will replicate w and distribute every row in xs to a device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpiH0-S3Hsy7",
        "outputId": "39e55f9a-9d02-419d-9e2b-7c92c7102291"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ShardedDeviceArray([[ 11.,  20.,  29.],\n",
              "                    [ 56.,  65.,  74.],\n",
              "                    [101., 110., 119.],\n",
              "                    [146., 155., 164.],\n",
              "                    [191., 200., 209.],\n",
              "                    [236., 245., 254.],\n",
              "                    [281., 290., 299.],\n",
              "                    [326., 335., 344.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* One thing to notice here is that now we have '**Sharded Device Arrays**' as an output instead if Device Array. This means that the data as well as the computation is sharded or distributed across multiple cores.\n",
        "\n",
        "* Another thing to note here is that if we were to run another parallel computation, the elements would stay on their respective devices and that there won't be any cost incurred for moving data between devices.\n",
        "\n",
        "* One more interesting thing to note here is that ```jax.pmap()``` does ```jit``` compile of the code additionally as part of its operations.\n",
        "\n",
        "* In this example the array of five elements was distributed on every device, along with the single Kernel, there is **no communication happening across the multiple cores**."
      ],
      "metadata": {
        "id": "boDlYTIUEgdS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can choose while parallelizing the computation to have a communication between the multiple cores or not. We might have to train our Neural Netwrok model in a distributed fashion in which every core would receive a batch of data and then they will compute, communicate and cordinate among them to get the mean of gradients to update our ML model.\n",
        "\n",
        "See below example for calculating the normalized values across each device so that the sum is 1. Here we need to pass the information across the devices."
      ],
      "metadata": {
        "id": "uQ2aBJY6N4Ds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Using same convolution example \n",
        "#Comunicate across devices this time\n",
        "def normalized_convolve(w,x):\n",
        "  output = []\n",
        "  for i in range(1,len(x)-1):\n",
        "    output.append(jnp.dot(x[i-1:i+2],w))\n",
        "  output = jnp.array(output)\n",
        "\n",
        "  return output / jax.lax.psum(output,axis_name= 'p')  #Trying to communicate across devices\n",
        "\n",
        "jax.pmap(normalized_convolve,axis_name='p', in_axes=(None,0))(w,xs) #If you will sum all the row values for any column you will get 1\n",
        "#Note the Sharded Device Array as the output "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6JvRe6E3Oe-I",
        "outputId": "1d809698-03a2-4922-a092-c474515a3e83"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ShardedDeviceArray([[0.00816024, 0.01408451, 0.019437  ],\n",
              "                    [0.04154303, 0.04577465, 0.04959785],\n",
              "                    [0.07492582, 0.07746479, 0.07975871],\n",
              "                    [0.10830861, 0.10915492, 0.10991956],\n",
              "                    [0.14169139, 0.14084506, 0.14008042],\n",
              "                    [0.17507419, 0.17253521, 0.17024128],\n",
              "                    [0.20845698, 0.20422535, 0.20040214],\n",
              "                    [0.24183977, 0.23591548, 0.23056298]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**value_and_grad()** function \n",
        "* It conveniently gives you both the function's value as well as its gradient.\n",
        "* It will help in creating the model training pipeline using pure jax while getting the value as well as the gradient of the loss function. "
      ],
      "metadata": {
        "id": "aHYYsO9oTtlS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from jax._src.api import value_and_grad\n",
        "#Simple loss function - sum of square of error differences \n",
        "def sum_squared_error(x,y):\n",
        "  return sum((x-y)**2)  #derivative wrto x will be 2(x-y)\n",
        "\n",
        "x = jnp.arange(4, dtype=jnp.float32)\n",
        "y = x+0.4\n",
        "print(\"x:\",x,\", y:\",y, \", loss:\", sum_squared_error(x,y))\n",
        "\n",
        "#Calculating the loss value as well as gradient\n",
        "jax.value_and_grad(sum_squared_error)(x,y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFaMho04ZiNX",
        "outputId": "bfca4914-28fb-490e-ed47-51f7dcde8df0"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x: [0. 1. 2. 3.] , y: [0.4 1.4 2.4 3.4] , loss: 0.6400001347064972\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(DeviceArray(0.64000016, dtype=float32),\n",
              " DeviceArray([-0.8       , -0.79999995, -0.8000002 , -0.8000002 ], dtype=float32))"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Working towards training a Neural Network**\n",
        "\n",
        "Training a model completly in parallel. \n",
        "If you want to log both, your loss value and your gradients of the loss then you can us jax.value_and_grad() inbuilt function."
      ],
      "metadata": {
        "id": "2jyPbYA5kPpy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "All functions in jax are implemented as composale program/function transformations.\n",
        "\n",
        "vmap automatically batches your code.\n"
      ],
      "metadata": {
        "id": "dPe6mPb5fXF3"
      }
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "JAX_3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPTdG6Etm6LBUPUz6T48kwS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yvrjsharma/JAX/blob/main/JAX_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# JAX: PMaps"
      ],
      "metadata": {
        "id": "JGT5gMrmnvR1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "JAX has lots of cool features to evaluate your code parallely. This is also called SPMD, or Single-Program Multiple-Data code. In this technique same code or computation is run in parallel on different input data on different devices (e.g. TPUs)\n",
        "\n",
        "You can use pmap() to write a piece of code suitable for running on one device as well as on multiple devices.\n",
        "\n"
      ],
      "metadata": {
        "id": "EQJMUGsm0ZQt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets start using TPUs for this one. If you are using this Colab notebook, make sure you change your Runtime to TPU."
      ],
      "metadata": {
        "id": "iNH0-tjBn1gC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Lets import the required libraries\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "\n",
        "#transformation : higher order functions which take fuun as an input and outputs a transformed fun \n",
        "from jax import grad, jit, vmap, pmap\n",
        "from jax import random\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from copy import deepcopy\n",
        "from typing import Tuple, NamedTuple\n",
        "import functools"
      ],
      "metadata": {
        "id": "iar5nrp18j5p"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.tools.colab_tpu\n",
        "jax.tools.colab_tpu.setup_tpu()"
      ],
      "metadata": {
        "id": "91_VTK8y4jAu"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "jax.devices() #Eight devices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Qcoby81odla",
        "outputId": "f0560537-83fb-4623-f3d8-12c7fbfa60ca"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0),\n",
              " TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1),\n",
              " TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0),\n",
              " TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1),\n",
              " TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0),\n",
              " TpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1),\n",
              " TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0),\n",
              " TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to ustilize all the cores that are available to us, lets create some array with batch size equal to the number of cores available.\n",
        "\n",
        "Lets now perform a dummy computation - convolving a small array over this array which is spread across the cores. This dummy example will help us in understanding later how pmap() helps in speeding up more complex or bigger computations.  "
      ],
      "metadata": {
        "id": "QT9QI_hjLfY0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_devices = jax.local_device_count(  )\n",
        "xs = np.arange(5*n_devices).reshape(-1,5)\n",
        "w = np.array([2.,3.,4.])  #for convolving over the given metric\n",
        "ws = np.stack([w] * n_devices)  #duplicating the same convolution kernel on 8 cores\n",
        "xs, ws"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xo6AsUrC8MIL",
        "outputId": "ed3d5484-4535-40f4-9564-fc361066fea8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[ 0,  1,  2,  3,  4],\n",
              "        [ 5,  6,  7,  8,  9],\n",
              "        [10, 11, 12, 13, 14],\n",
              "        [15, 16, 17, 18, 19],\n",
              "        [20, 21, 22, 23, 24],\n",
              "        [25, 26, 27, 28, 29],\n",
              "        [30, 31, 32, 33, 34],\n",
              "        [35, 36, 37, 38, 39]]), array([[2., 3., 4.],\n",
              "        [2., 3., 4.],\n",
              "        [2., 3., 4.],\n",
              "        [2., 3., 4.],\n",
              "        [2., 3., 4.],\n",
              "        [2., 3., 4.],\n",
              "        [2., 3., 4.],\n",
              "        [2., 3., 4.]]))"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def convolve(w,x):\n",
        "  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uow4dnowMt6s",
        "outputId": "829bfe76-da97-4d36-ca1d-f4d959acac0e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 2., 3.],\n",
              "       [1., 2., 3.]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To train big NNs we need to parallelize our computation. This is where pmap() comes into the picture.\n",
        "\n",
        "Interesting thing is if you shard or distribute a *device array* computation across multiple cores or accelerators or devices you create what is called **Sharded Device Array**.\n",
        "\n",
        "We can choose while parallelizing the computation to have a communication between the multiple cores or not. We can train our Neural Netwrok model in a distributed fashion in which every core will receive a batch of data and then they will communicate and cordinate among them to get the mean of gradients to update our ML model.\n",
        "\n",
        "Also, note that pmap() calls jit() internally on computations."
      ],
      "metadata": {
        "id": "uQ2aBJY6N4Ds"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Working towards training a Neural Network**\n",
        "\n",
        "If you want to log both, your loss value and your gradients of the loss then you can us jax.value_and_grad() inbuilt function."
      ],
      "metadata": {
        "id": "2jyPbYA5kPpy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "All functions in jax are implemented as composale program/function transformations.\n",
        "\n",
        "vmap automatically batches your code.\n"
      ],
      "metadata": {
        "id": "dPe6mPb5fXF3"
      }
    }
  ]
}
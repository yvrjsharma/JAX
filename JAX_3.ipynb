{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "JAX_3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPGqfcrsXIcRgsCs9jtMQ6a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yvrjsharma/JAX/blob/main/JAX_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# JAX: PMaps"
      ],
      "metadata": {
        "id": "JGT5gMrmnvR1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "JAX has lots of cool features to evaluate your code parallely. This is also called SPMD, or Single-Program Multiple-Data code. In this technique same code or computation is run in parallel on different input data on different devices (e.g. TPUs)\n",
        "\n",
        "You can use pmap() to write a piece of code suitable for running on one device as well as on multiple devices.\n",
        "\n"
      ],
      "metadata": {
        "id": "EQJMUGsm0ZQt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets start using TPUs for this one. If you are using this Colab notebook, make sure you change your Runtime to TPU."
      ],
      "metadata": {
        "id": "iNH0-tjBn1gC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Lets import the required libraries\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "\n",
        "#transformation : higher order functions which take fuun as an input and outputs a transformed fun \n",
        "from jax import grad, jit, vmap, pmap\n",
        "from jax import random\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from copy import deepcopy\n",
        "from typing import Tuple, NamedTuple\n",
        "import functools"
      ],
      "metadata": {
        "id": "iar5nrp18j5p"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.tools.colab_tpu\n",
        "jax.tools.colab_tpu.setup_tpu()"
      ],
      "metadata": {
        "id": "91_VTK8y4jAu"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "jax.devices() #Eight devices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Qcoby81odla",
        "outputId": "79f74f33-2ed4-46a9-a714-ebfa7fd4d53c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0),\n",
              " TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1),\n",
              " TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0),\n",
              " TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1),\n",
              " TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0),\n",
              " TpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1),\n",
              " TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0),\n",
              " TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to ustilize all the cores that are available to us, lets create some array with batch size equal to the number of cores available.\n",
        "\n",
        "Lets now perform a dummy computation - convolving a small array over this array which is spread across the cores. This dummy example will help us in understanding later how pmap() helps in speeding up more complex or bigger computations.  "
      ],
      "metadata": {
        "id": "QT9QI_hjLfY0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_devices = jax.local_device_count()\n",
        "x = np.arange(5)\n",
        "xs = np.arange(5*n_devices).reshape(-1,5)  #40 unique values, arranged in 5 columns and 8 rows\n",
        "w = np.array([2.,3.,4.])  #for convolving over the given metric\n",
        "ws = np.stack([w] * n_devices)  #duplicating the same convolution kernel on 8 cores\n",
        "x,w , xs, ws"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xo6AsUrC8MIL",
        "outputId": "c4099362-77b8-4298-9e4f-386c96a7afda"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0, 1, 2, 3, 4]), array([2., 3., 4.]), array([[ 0,  1,  2,  3,  4],\n",
              "        [ 5,  6,  7,  8,  9],\n",
              "        [10, 11, 12, 13, 14],\n",
              "        [15, 16, 17, 18, 19],\n",
              "        [20, 21, 22, 23, 24],\n",
              "        [25, 26, 27, 28, 29],\n",
              "        [30, 31, 32, 33, 34],\n",
              "        [35, 36, 37, 38, 39]]), array([[2., 3., 4.],\n",
              "        [2., 3., 4.],\n",
              "        [2., 3., 4.],\n",
              "        [2., 3., 4.],\n",
              "        [2., 3., 4.],\n",
              "        [2., 3., 4.],\n",
              "        [2., 3., 4.],\n",
              "        [2., 3., 4.]]))"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Simple convolution example \n",
        "def convolve(w,x):\n",
        "  output = []\n",
        "  for i in range(1,len(x)-1):\n",
        "    output.append(jnp.dot(x[i-1:i+2],w))\n",
        "  return jnp.array(output)\n",
        "\n",
        "convolve(w,x) #smaller array is convolved over another one"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uow4dnowMt6s",
        "outputId": "544a4045-657b-4781-f817-37c33359198b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray([11., 20., 29.], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Lets first try the convolve operation over xs using just one device\n",
        "#We can do so using vmap()\n",
        "#convolve(ws,xs) #Error for incompatible shapes\n",
        "jax.vmap(convolve)(ws,xs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIUsngIZDW0U",
        "outputId": "2609b45b-ff4f-4cce-f587-c1b3dd6cabcc"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray([[ 11.,  20.,  29.],\n",
              "             [ 56.,  65.,  74.],\n",
              "             [101., 110., 119.],\n",
              "             [146., 155., 164.],\n",
              "             [191., 200., 209.],\n",
              "             [236., 245., 254.],\n",
              "             [281., 290., 299.],\n",
              "             [326., 335., 344.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Coolest thing in JAX is that you don't hav to change the function \n",
        "#When you need to run it on multiple devices\n",
        "#Using pmap() this time\n",
        "jax.pmap(convolve)(ws,xs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_INhgO2GEKcd",
        "outputId": "08de915d-f388-4fcb-812c-6694ed995a82"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ShardedDeviceArray([[ 11.,  20.,  29.],\n",
              "                    [ 56.,  65.,  74.],\n",
              "                    [101., 110., 119.],\n",
              "                    [146., 155., 164.],\n",
              "                    [191., 200., 209.],\n",
              "                    [236., 245., 254.],\n",
              "                    [281., 290., 299.],\n",
              "                    [326., 335., 344.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Another way of writing above code\n",
        "#Using in_axes argument of pmap() \n",
        "#Works similar to vmap()\n",
        "jax.pmap(convolve,in_axes=(None,0))(w,xs)  #Using w instead of ws\n",
        "\n",
        "#'None' would mean to broadcast weight w across all the cores \n",
        "#'0' would mean that 0 dimension in xs matrix is the batch argument\n",
        "#So, pmap will replicate w and distribute every row in xs to a device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpiH0-S3Hsy7",
        "outputId": "39e55f9a-9d02-419d-9e2b-7c92c7102291"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ShardedDeviceArray([[ 11.,  20.,  29.],\n",
              "                    [ 56.,  65.,  74.],\n",
              "                    [101., 110., 119.],\n",
              "                    [146., 155., 164.],\n",
              "                    [191., 200., 209.],\n",
              "                    [236., 245., 254.],\n",
              "                    [281., 290., 299.],\n",
              "                    [326., 335., 344.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* One thing to notice here is that now we have '**Sharded Device Arrays**' as an output instead if Device Array. This means that the data as well as the computation is sharded or distributed across multiple cores.\n",
        "\n",
        "* Another thing to note here is that if we were to run another parallel computation, the elements would stay on their respective devices and that there won't be any cost incurred for moving data between devices.\n",
        "\n",
        "* One more interesting thing to note here is that ```jax.pmap()``` does ```jit``` compile of the code additionally as part of its operations.\n",
        "\n",
        "* In this example the array of five elements was distributed on every device, along with the single Kernel, there is **no communication happening across the multiple cores**."
      ],
      "metadata": {
        "id": "boDlYTIUEgdS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can choose while parallelizing the computation to have a communication between the multiple cores or not. We might have to train our Neural Netwrok model in a distributed fashion in which every core would receive a batch of data and then they will compute, communicate and cordinate among them to get the mean of gradients to update our ML model.\n",
        "\n",
        "See below example for calculating the normalized values across each device so that the sum is 1. Here we need to pass the information across the devices."
      ],
      "metadata": {
        "id": "uQ2aBJY6N4Ds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Using same convolution example \n",
        "#Comunicate across devices this time\n",
        "def normalized_convolve(w,x):\n",
        "  output = []\n",
        "  for i in range(1,len(x)-1):\n",
        "    output.append(jnp.dot(x[i-1:i+2],w))\n",
        "  output = jnp.array(output)\n",
        "\n",
        "  return output / jax.lax.psum(output,axis_name= 'p')  #Trying to communicate across devices\n",
        "\n",
        "jax.pmap(normalized_convolve,axis_name='p', in_axes=(None,0))(w,xs) #If you will sum all the row values for any column you will get 1\n",
        "#Note the Sharded Device Array as the output "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6JvRe6E3Oe-I",
        "outputId": "1d809698-03a2-4922-a092-c474515a3e83"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ShardedDeviceArray([[0.00816024, 0.01408451, 0.019437  ],\n",
              "                    [0.04154303, 0.04577465, 0.04959785],\n",
              "                    [0.07492582, 0.07746479, 0.07975871],\n",
              "                    [0.10830861, 0.10915492, 0.10991956],\n",
              "                    [0.14169139, 0.14084506, 0.14008042],\n",
              "                    [0.17507419, 0.17253521, 0.17024128],\n",
              "                    [0.20845698, 0.20422535, 0.20040214],\n",
              "                    [0.24183977, 0.23591548, 0.23056298]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**value_and_grad()** function \n",
        "* It conveniently gives you both the function's value as well as its gradient.\n",
        "* It will help in creating the model training pipeline using pure jax while getting the value as well as the gradient of the loss function. "
      ],
      "metadata": {
        "id": "aHYYsO9oTtlS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from jax._src.api import value_and_grad\n",
        "#Simple loss function - sum of square of error differences \n",
        "def sum_squared_error(x,y):\n",
        "  return sum((x-y)**2)  #derivative wrto x will be 2(x-y)\n",
        "\n",
        "x = jnp.arange(4, dtype=jnp.float32)\n",
        "y = x+0.4\n",
        "print(\"x:\",x,\", y:\",y, \", loss:\", sum_squared_error(x,y))\n",
        "\n",
        "#Calculating the loss value as well as gradient\n",
        "jax.value_and_grad(sum_squared_error)(x,y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFaMho04ZiNX",
        "outputId": "bfca4914-28fb-490e-ed47-51f7dcde8df0"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x: [0. 1. 2. 3.] , y: [0.4 1.4 2.4 3.4] , loss: 0.6400001347064972\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(DeviceArray(0.64000016, dtype=float32),\n",
              " DeviceArray([-0.8       , -0.79999995, -0.8000002 , -0.8000002 ], dtype=float32))"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Training a Neural Network using parallelism of JAX**\n",
        "\n",
        "* Training a model completly in parallel. \n",
        "* Each batch of data will be split into sub-batches which are evaluated on separate devices.\n",
        "\n",
        "Lets's start by creating some dummy data. We will try and fit a line to this data for our example --"
      ],
      "metadata": {
        "id": "2jyPbYA5kPpy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Lets create some data for the function with some noise => y = w*x + b +noise \n",
        "true_weights , true_bias = 2,-1\n",
        "xs = np.random.normal(size=(128,1))\n",
        "noise =  0.5*np.random.normal(size=(128,1))\n",
        "ys = xs * true_weights + true_bias + noise\n",
        "\n",
        "#Lets plot this linear distributed data\n",
        "plt.scatter(xs,ys)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "lDFq9BSaZgpA",
        "outputId": "c3c74a93-15ce-4c72-e299-88c4bcdd96de"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "dsdsdsdsd"
      ],
      "metadata": {
        "id": "qNgYp66-cCJ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating a params class as a named tuple class \n",
        "class Params(NamedTuple):\n",
        "  #Values are jax arrays\n",
        "  weight: jnp.ndarray\n",
        "  bias: jnp.ndarray\n",
        "\n",
        "lr = 0.005"
      ],
      "metadata": {
        "id": "R9vmu__0f1UM"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#For iniitializing the model\n",
        "#intializing rndom values to the weight and bias for our model\n",
        "def init_model(rng):\n",
        "  weights_key, bias_key = jax.random.split(rng)\n",
        "  weight = jax.random.normal(weights_key,())\n",
        "  bias = jax.random.normal(bias_key,())\n",
        "  return Params(weight,bias)  #return as Params class values\n"
      ],
      "metadata": {
        "id": "tAv49yBkcJsd"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Intialize the parameteres and replicate across devices\n",
        "#to do the coputation in parallel\n",
        "params = init_model(jax.random.PRNGKey(1))\n",
        "n_devices=jax.local_device_count()\n",
        "replicated_params = jax.tree_map(lambda x: jnp.array([x]*n_devices), params)\n",
        "print(replicated_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q2M5_7bzhd2l",
        "outputId": "5d737a3d-4866-4d34-b2a3-dc5ca6c25e88"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Params(weight=DeviceArray([-1.1521945, -1.1521945, -1.1521945, -1.1521945, -1.1521945,\n",
            "             -1.1521945, -1.1521945, -1.1521945], dtype=float32), bias=DeviceArray([-1.1470189, -1.1470189, -1.1470189, -1.1470189, -1.1470189,\n",
            "             -1.1470189, -1.1470189, -1.1470189], dtype=float32))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Reshape our normal data (generated above) so that we can use it with pmap() in parallel\n",
        "#Split the data equally between the devices \n",
        "#such that the leading dimension of these arrays  is n_devices (or 8 in our case)\n",
        "def reshape_for_pmap(data, n_devices):\n",
        "  return data.reshape(n_devices, data.shape[0]//n_devices,*data.shape[1:])\n",
        "\n",
        "x_parallel = reshape_for_pmap(xs, n_devices)\n",
        "y_parallel = reshape_for_pmap(ys, n_devices)\n",
        "\n",
        "print(x_parallel.shape, xs.shape)\n",
        "#This shape is now suitable for using pmap"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2V8yqD4kxZD",
        "outputId": "d5ba048c-a498-4d01-e406-6898e24671d3"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8, 16, 1) (128, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I trained an MLP in my last post and rest of the architecture is almost similar to that. We are going to iterate our model for 1000 epochs this time."
      ],
      "metadata": {
        "id": "nYd3AZ1IqdDV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Compute least squares error of the model prediction on xs and ys\n",
        "def loss_fn(params, xs, ys):\n",
        "  pred = params.weight*xs + params.bias\n",
        "  return jnp.mean((pred - ys)**2)\n",
        "\n",
        "\n",
        "#Performs SGD updates over the params using given data\n",
        "@functools.partial(jax.pmap,axis_name='p')\n",
        "def update(params, xs, ys):\n",
        "\n",
        "  #compute the gradients on the given minibatch (individually on each device)\n",
        "  loss, grads = jax.value_and_grad(loss_fn)(params, xs,ys)\n",
        "\n",
        "  #compute the gradient across all devices \n"
      ],
      "metadata": {
        "id": "YQsi59aumIZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All functions in jax are implemented as composale program/function transformations.\n",
        "\n",
        "vmap automatically batches your code.\n",
        "\n",
        "\n",
        "If you want to log both, your loss value and your gradients of the loss then you can us jax.value_and_grad() inbuilt function.\n"
      ],
      "metadata": {
        "id": "dPe6mPb5fXF3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "tyCzHlwUWWiw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}